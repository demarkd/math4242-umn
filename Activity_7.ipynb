{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5923f7c0-2bcb-46a3-85c8-028ea6e0aacb",
   "metadata": {},
   "source": [
    "# Activity 7: SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a930da2-2419-4bc3-9683-e169786abd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db848c4-c79a-4fd2-a3db-140c4d211d44",
   "metadata": {},
   "source": [
    "### Exercise 1: SVD\n",
    "Let $A$ be an $m\\times n$ matrix of rank $r$.\n",
    "Recall that the *singular values* of $A$ are the positive square roots $\\sigma_i=\\sqrt{\\lambda_i}>0$ of the eigenvalues $\\lambda_i$ of $K=A^T A$.\n",
    "A *singular value decomposition* (SVD) of $A$ is a factorization $A=P\\Sigma Q^T$ where $P$ is an $m\\times r$ matrix with orthonormal columns, $\\Sigma$ is an $r\\times r$ diagonal matrix with the singular values of $A$ as its entries, and $Q^T$ is $r\\times n$ with orthonormal rows (i.e. $Q$ is $n\\times r$ with orthonormal columns. \n",
    "By convention, we take $\\Sigma$ to have its diagonal entries ordered from greatest to least.\n",
    "It is Theorem 8.63 of Olver-Shakiban that every matrix has a SVD.\n",
    "The proof of that theorem contains a recipe for finding the SVD of $A$:\n",
    "- Let $\\vec q_1,\\dots \\vec q_r$ be the orthonormal eivgenvectors of $K=A^TA$ such that each $q_i$ corresponds to $\\lambda_i=\\sigma_i^2$.\n",
    "- Then, set $\\vec p_i=\\frac{A \\vec q_i}{\\sigma_i}$\n",
    "- Let $P$ be the matrix with $\\vec p_i$ as its $i$th column, and let $Q^T$ be the matrix with $\\vec q_i$ as its $i$th row.\n",
    "\n",
    "**Exercise:** Below, give a function `my_SVD` which produces a SVD for any input matrix $A$. \n",
    "While you can use `symmetric_eigensolver` from last activity to do so, you might run into accuracy issues if you do. \n",
    "Instead, consider using the equivalent built-in function, `np.linalg.eigh` (imported for convenience as `LA.eigh`).\n",
    "`LA.eigh` gives output in the same format as `symmetric_eigensolver`, but with eigenvalue ascending rather than descending.\n",
    "You may be tempted to use `np.count_nonzero`, but there is an issue there--while `LA.eigh` is significantly more accurate than our `symmetric_eigensolver`, it still is not perfect and may return very small nonzero values instead of zero. \n",
    "Instead, you should take an optional `tolerance` argument in your input and only take the eigenvalues greater than `tolerance`, ignoring the rest.\n",
    "Your function should output a tuple `(P,Sigma, Qt)` where `Qt` is the matrix $Q^T$.\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "<b>\n",
    "    Hints:\n",
    "</b>\n",
    "    (Click here to open)\n",
    "</summary>\n",
    "\n",
    "- One helpful built-in function is `np.linalg.diag` (here `LA.diag`) which converts vectors to diagonal matrices and vice-versa.\n",
    "- You may want to make an entire function (say, `count_nonzero_up_to_tolerance`) which counts the number of entries in a vector exceeding `tolerance` in order to determine rank.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9503364-8c8e-43bf-9789-0074864e53d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_SVD():\n",
    "    #YOUR CODE HERE\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423f0487-cce5-43c0-bcc9-c4da650d54f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing:\n",
    "A=np.array([[1,2,3,4],[2,3,4,5]],\"float64\")\n",
    "P,Sigma,Qt=my_SVD(A)\n",
    "P,Sigma,Qt\n",
    "#Desired Output: (up to possibly simultaneously negating columns of P,Qt) \n",
    "#(array([[-0.59693053,  0.80229293],\n",
    "#        [-0.80229293, -0.59693053]]),\n",
    "# array([[9.15211593, 0.        ],\n",
    "#        [0.        , 0.48864503]]),\n",
    "# array([[-0.24054726, -0.3934325 , -0.54631774, -0.69920298],\n",
    "#        [-0.80133452, -0.38106544,  0.03920365,  0.45947273]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874968e9-c7cb-41ae-9281-0693816bec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Further testing:\n",
    "np.dot(np.dot(P,Sigma),Qt)\n",
    "#what should the desired output be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b92534d-8bf9-4279-a22a-88798b9ba3e1",
   "metadata": {},
   "source": [
    "## Exercise 2: Pseudoinverses.\n",
    "\n",
    "Recall that the pseudo inverse of a matrix $A$ with SVD $A=P\\Sigma Q^T$ is $A^+=Q\\Sigma^{-1}P^T$.\n",
    "\n",
    "**Exercise: (a)** Below, use `my_SVD` to give the pseudoinverse of an input matrix `A`. Do **not** use built-in functions to compute an inverse for $\\Sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190b9ecf-9119-4791-9d59-4b3fd224ba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_pseudoinv():\n",
    "    #YOUR CODE HERE\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b944e017-8f6d-4ddc-be74-4eb4a4768086",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing \n",
    "A1=np.array([[1,3],[1,4]])\n",
    "A1_plus=my_pseudoinv(A1)\n",
    "np.dot(A1,A1_plus)\n",
    "#What should we expect for output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e25036-5a21-4308-9c03-46b10b24b200",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing \n",
    "A=np.array([[1,2,3,4],[2,3,4,5]],\"float64\")\n",
    "my_pseudoinv(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1183954-54cc-4f93-b241-fc2f2e30aaef",
   "metadata": {},
   "source": [
    "Pseudoinverses are useful in part because they assist in optimization problems. \n",
    "In particular, consider the least squares problem $A\\vec x =\\vec b$ and let $\\vec x^*=A^+\\vec b$. \n",
    "Then, by Lemma 8.68, $\\vec x^*=(A^TA)^{-1}A^T\\vec b$, and so in particular, $\\vec x^*$ solves the normal equations as $A^TA\\vec x^*=A^TA A^+\\vec b=(Q\\Sigma^2Q^T)(Q\\Sigma^{-1}P^T) \\vec b=Q\\Sigma P^T \\vec b=A^T \\vec b$.\n",
    "\n",
    "**Exercise (b):** Below, give a function `least_squares2` which takes as input a matrix `A`, a vector `b` and a `tolerance` and solves the least squares problem $A\\vec x=\\vec b$, returning the minimizer $\\vec x^*$ and the minimum $\\left \\lVert A\\vec x-\\vec b\\right \\rVert$, passing the tolerance to `my_SVD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfd9d3f-7a30-4314-b8e5-c42b860ff9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares2():\n",
    "    #YOUR CODE HERE\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0159956-35cd-45fb-8536-ddc4f0176c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "A2=np.array([\n",
    "    [1,2,-1],\n",
    "    [3,-4,1],\n",
    "    [-1,3,-1],\n",
    "    [2,-1,0]\n",
    "],\"float64\")\n",
    "b=np.array([1,0,-1,2],\"float64\")\n",
    "least_squares2(A2,b)\n",
    "#desired output:\n",
    "#(array([ 0.56666667,  0.13333333, -0.16666667]), 1.7320508075688772)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c8d292-c573-42c5-8ff1-e36c09ca7348",
   "metadata": {},
   "source": [
    "## Exercise 3: Condition Numbers and Hilbert's Revenge\n",
    "Recall our folly with *Hilbert Matrices* earlier in the semester.\n",
    "These were the matrix $H_n=\\left(\\frac1{i+j-1}\\right)_{i,j=1}^n$.\n",
    "We saw that they were particularly resistant to analysis by LU, which was attributed (rather off-handedly) to being \"ill-conditioned.\"\n",
    "Now, to finally come back around and put a ribbon on that, let's explore ill-conditionedness in better detail.\n",
    "\n",
    "**Definition:** The condition number of a rank-$r$ matrix $A$ is the ratio of the greatest singular value to the least, that is $\\sigma_1/\\sigma_r$.\n",
    "\n",
    "**Exercise: (a)** Below, write a function which uses `my_SVD` to compute the condition number of an input matrix.\n",
    "Your function should again take a tolerance argument which is passed to `my_SVD` (this is very important here).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145853bd-8b5d-450e-a7ef-72de2057146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition_number():\n",
    "    #YOUR CODE HERE\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583ae667-dd97-4838-b6bd-173ad37fcdd3",
   "metadata": {},
   "source": [
    "**(b)** Copy or rewrite or otherwise re-load your `hilbert` function from before. \n",
    "By any method you like, Generate a list of condition numbers of $H_n$ with $n=2,\\dots,20$ using `tolerance=10**-10`.\n",
    "What do you notice? Is this in line with your expectations, or does it run counter to your expectations.\n",
    "What happens if you change the tolerance? How do your results seem to respond?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d433bc-1175-4bb6-8fa6-d50cf1f473d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hilbert():\n",
    "    #YOUR CODE HERE\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a7c3f6-bde8-4200-977a-c8d80e01b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE TO GENERATE CONDITION NUMBERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75285d89-85e9-45e0-8e84-45736e16a8a4",
   "metadata": {},
   "source": [
    "(This is a markdown cell to write down your observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b27b7b-ef91-469b-a283-0f5e64979e72",
   "metadata": {},
   "source": [
    "**(c)** It is known that for every $n\\geq 2$, $H_n$ is invertible (and hence full rank). \n",
    "Rewrite your code for part (b) to also print the observed rank in addition to the observed condition numbers (possibly also playing around with `tolerance`).\n",
    "Do your results agree with what is known theoretically? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39b255b-7e11-4ca2-a9df-bda4901fde53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def condition_number_and_rank(:\n",
    "    #YOUR CODE HERE\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe033d6-cb2c-48f7-ac79-6282f9d21654",
   "metadata": {},
   "source": [
    "(This is a markdown cell to write down your observations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5983e567-4d0e-43ab-88c4-b3815149f9a1",
   "metadata": {},
   "source": [
    "**(d)** Note that for a symmetric matrix $K$, the singular values $\\sigma_i$ are exactly the eigenvalues of $K$. Use this fact and `LA.eigh` to attempt once more to compute the actual condition numbers of $H_n$ for whichever values you are able to. Do your results make sense? If not, where do they seem to diverge from what you expect? Write a few words about your observations. What do you think is happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339979db-3768-41ac-b618-1e7d175fea54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition_number_symmetric():\n",
    "    #YOUR CODE HERE\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19d8b92-25cc-4669-b3a3-14bab711892a",
   "metadata": {},
   "source": [
    "(This is a markdown cell to write down your observations)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
