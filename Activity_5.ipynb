{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c74206c0",
   "metadata": {},
   "source": [
    "# Activity 5: Minimization Problems and Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188d95c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cac7f3f",
   "metadata": {},
   "source": [
    "## Exercise 1: Some Utility Functions\n",
    "### Positive Definite Matrices\n",
    "Recall that a symmetric $n\\times n$ matrix $K$ is positive semi-definite if $\\vec x^T K \\vec x\\geq 0$ for all $\\vec x\\in \\mathbb R ^ n$, and positive definite if $\\vec x^T K \\vec x> 0$ for all $\\vec 0 \\neq\\vec x\\in \\mathbb R ^ n$.\n",
    "A very helpful characterization of positive definite matrices comes from the following theorem in Olver-Shakiban:\n",
    "\n",
    "**Theorem 3.43.** A symmetric matrix is positive definite if and only if it is regular and has all positive pivots.\n",
    "\n",
    "**Exercise: (a)** Below, give a function `is_pos_def` which determines if a matrix `A` is positive definite. We'll do this by checking that:\n",
    "- `A` is symmetric.\n",
    "- `A` is regular\n",
    "- `A` has all positive pivots\n",
    "Two of these properties are relatively unstable, so we will work up to a specified tolerance, set by default to (say) `10**(-10)`. That is, we'll say `A` is symmetric up to `tolerance` if `abs(A[i,j]-A[j,i])<tolerance`, and that it has positive pivots if each pivot is greater than the supplied tolerance.\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary> \n",
    "        <b> Hint:</b> (click to expand)\n",
    "    </summary>\n",
    "A few built-in functions may be helpful\n",
    "- `np.any` takes an array of booleans as input and returns `True` if every entry is `True`\n",
    "- `np.array_equal` checks if two arrays are equal on the nose.\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dfaa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pos_def():\n",
    "    #YOUR CODE HERE\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840578dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "B=np.array([[1,2,3,4],[4,5,6,7],[-1,2,-3,4]],\"float64\")\n",
    "K1=np.dot(B,np.transpose(B))\n",
    "K2=np.dot(np.transpose(B),B)\n",
    "K1, is_pos_def(K1),K2,is_pos_def(K2)\n",
    "#What should be the correct output? Note rank(B)=3..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f10945-7c7d-497f-ad48-60d3f00816fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing pt 2\n",
    "L=np.array([[1,1,1,1.],\n",
    "[1,-1,1.,-1],\n",
    "[1,1,-1.,-1],\n",
    "[-1,1,1.,-1]],\"float64\")/2\n",
    "D=np.diag([10**-2,10**-4,10**-10,10**-12])\n",
    "K3= np.dot(L,np.dot(D,np.transpose(L)))\n",
    "K3,is_pos_def(K3),is_pos_def(K3,10**-13)\n",
    "# desired output: \n",
    "#(array([[ 0.002525,  0.002475,  0.002525, -0.002475],\n",
    "#        [ 0.002475,  0.002525,  0.002475, -0.002525],\n",
    "#        [ 0.002525,  0.002475,  0.002525, -0.002475],\n",
    "#        [-0.002475, -0.002525, -0.002475,  0.002525]]),\n",
    "# False,\n",
    "# True)\n",
    "# why does this make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dfecdb",
   "metadata": {},
   "source": [
    "### Minimization Problems\n",
    "In section 5.2 of Olver-Shakiban, we learned a systematic approach for minimizing quadratic functions.\n",
    "These are functions of the form $p(\\vec x) = \\vec x^T K \\vec x -2\\vec x^T \\vec f +c$, where $K$ is a symmetric $n\\times n$ matrix.\n",
    "Another theorem gives the minimizer and minimum:\n",
    "\n",
    "**Theorem 5.2.** If $K$ is a positive definite (and hence symmetric) matrix, then the quadratic function above has a unique minimizer, namely the solution $\\vec x^*$ to the linear system $K \\vec x =\\vec f$. The minimum value $p(\\vec x^*)$ is equal to $c-\\vec f ^t \\vec x^*$.\n",
    "\n",
    "**(b)** Below, give a function which returns the minimizer and minimum of the quadratic function represented by the triple `K,f,c`.\n",
    "Use a solving function from a previous activity to solve any linear systems (and not a built-in function).\n",
    "Your function should `raise` an `Exception` if the supplied matrix fails to be positive definite (this will end up being important later)\n",
    "\n",
    "**(c)** Use this to solve exercise 5.2.1: Find the global minimum value and global minimizer of the function $f(x,y,z)=x^2+2xy+3y^2+2yz+z^2-2x+3z+2$. Remember that any new arrays that you instantiate should be `\"float64\"` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18faced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_min():\n",
    "    #YOUR CODE HERE\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e46b16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "K=np.array([\n",
    "    [1,1,1/2],\n",
    "    [1,2,1/2],\n",
    "    [1/2,1/2,2]\n",
    "],\"float64\")\n",
    "f=np.array([0,-3,7/2],\"float64\")\n",
    "c=5\n",
    "quadratic_min(K,f,c)\n",
    "#desired output (array([ 2., -3.,  2.]), -11.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161f34b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#part (c)\n",
    "    #YOUR CODE HERE\n",
    "\n",
    "#Answer: -12.0 at (-1,-1,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfd626f",
   "metadata": {},
   "source": [
    "## Exercise 2: Closest Point & Least Squares\n",
    "### Closest Point\n",
    "In the closest point problem, we are given a subspace $W\\subset \\mathbb{R}^m$ (typically with a basis given, say $\\vec w_1,\\dots \\vec w_n$) and a vector $\\vec b$ and asked to find the vector $\\vec w \\in W$ such that the distance $\\left \\lVert\\vec w-\\vec b \\right \\rVert$ is minimized.\n",
    "\n",
    "Since minimizing the square of a positive quantity is equivalent to minimizing the quantity itself, this is equivalent to minimizing $\\left \\lVert\\vec w-\\vec b \\right \\rVert^2=\\left \\langle \\vec w-\\vec b,\\vec w-\\vec b\\right \\rangle=\\left\\langle \\vec w,\\vec w\\right \\rangle -2\\left\\langle \\vec w,\\vec b\\right \\rangle+\\left \\lVert \\vec b\\right \\rVert^2$.\n",
    "Writing $\\vec w= x_1\\vec w_1 +\\dots +x_n\\vec w_n$ then gives the expression $\\left \\lVert \\vec w \\right \\rVert^2= \\sum_{i,j=1}^n k_{ij}x_ix_j=\\vec x^T K \\vec x$ where $K=(k_{ij})$ is the Gram matrix of the basis $\\vec w_1,\\dots, \\vec w_n$.\n",
    "We also note that the same trick of writing out $\\vec w$ in terms of the $\\vec w_i$ yields an expression $\\langle \\vec w, \\vec b \\rangle=\\sum x_i \\langle \\vec w_i, \\vec b\\rangle=\\vec x^T \\vec f$ where $f_i=\\langle \\vec w_i, \\vec b\\rangle$. In other words, we've expressed $\\left \\lVert\\vec w-\\vec b \\right \\rVert^2$ as the quadratic function $p(\\vec x) = \\vec x^T K \\vec x -2\\vec x^T \\vec f +c$ where $K$ and $\\vec f$ are given above and $c=\\left \\lVert \\vec b \\right \\rVert^2$. \n",
    "The minimizer $\\vec x^*$ to $p$ then gives the coefficients $x_1,\\dots,x_n$ for the closest point $\\vec w^*=x_1 \\vec w_1+\\dots +x_n \\vec w_n$.\n",
    "\n",
    "**Exercise (a):** Write a function which takes as input a matrix `W` whose columns are the vectors $\\vec w_1,\\dots \\vec w_n$ and a vector `b` and returns the closest point $\\vec w^*$ and its distance $\\left \\lVert \\vec w^* -\\vec b \\right \\rVert$. \n",
    "Do so by computing the appropriate objects `K`, `f` and `c`, passing those to `quadratic_min`, and transforming the output into appropriate form (for part of that, note that `np.sqrt` might be helpful).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0557de6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_point():\n",
    "    #YOUR CODE HERE\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60c12e3-e32b-43f9-bd99-27213b42be4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "W=np.transpose(np.array([\n",
    "    [1,2,-1],\n",
    "    [2,-3,-1]\n",
    "    ],\"float64\"))\n",
    "b=np.array([1,0,0],\"float64\")\n",
    "closest_point(W,b)\n",
    "#desired output:\n",
    "#(array([ 0.66666667, -0.06666667, -0.46666667]), 0.5773502691896257)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529ed1bb-a154-424b-9691-3657706e3401",
   "metadata": {},
   "source": [
    "### Least Squares\n",
    "The least squares problem for the system $A\\vec x=\\vec b$ is that of finding the $\\vec x^*$ which minimizes the difference between the two sides. \n",
    "Olver-Shakiban gives a theorem which computes $\\vec x^*$ directly:\n",
    "\n",
    "**Theorem 5.11.** Assume $\\ker A = \\{\\vec 0 \\}$. Then, the least squares solution to $A\\vec x = \\vec b$ under the Euclidean norm is $\\vec x^*$, the unique solution to the *normal equations*: $(A^T A) \\vec x = A^T \\vec b$.\n",
    "The *least squares error* is $\\left \\lVert A \\vec x^* - \\vec b\\right \\rVert^2$.\n",
    "\n",
    "**Exercise (b):** Modify your function from part (a) to give a function `least_squares` which provides the (more general) least squares solution and least squares error for a given matrix `A` and vector `b`. \n",
    "\n",
    "*Hint:* This should be a matter more-or-less of *removing* code from the previous function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2369b8df-bb19-4001-8000-49e591aae63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares():\n",
    "    #YOUR CODE HERE\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e9522b-a8a0-42f3-ae5e-08ccf3cd0a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing:\n",
    "A=np.array([\n",
    "    [1,2,0],\n",
    "    [3,-1,1],\n",
    "    [-1,2,1],\n",
    "    [1,-1,-2],\n",
    "    [2,1,-1]\n",
    "    ],\"float64\")\n",
    "b=np.array([1,0,-1,2,2],\"float64\")\n",
    "least_squares(A,b)\n",
    "#Desired Output: (array([ 0.4118705 ,  0.24820144, -0.95323741]), 0.0323741007194247)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b479034-b4fa-4b7e-ac0b-6b44fad95307",
   "metadata": {},
   "source": [
    "## Exercise 3: Data-fitting\n",
    "# TODO: consider writing more background\n",
    "Read through the beginning of section 5.5 on Data Fitting and Interpolation on pages 254-258 of Olver-Shakiban.\n",
    "\n",
    "**Exercise: (a)** Write a function `linear_fit` which takes as input two vectors `t` and `y`. Use `least_squares` to compute $\\alpha,\\beta$ and print \"Line of best fit is y=$\\alpha$+$\\beta$t\", then return $(\\alpha,\\beta)$. \n",
    "\n",
    "(So for instance, if you find $\\alpha=3.1415$ and $\\beta=2.7183$, your function should print \"Line of best fit is y= 3.1415 + 2.7183 t\").\n",
    "\n",
    "Then, use `linear_fit` to solve problem 5.5.3(b) in Olver-Shakiban (estimating the price of a home in 2005 and 2010). You may [optionally] want to do this by writing another function that wraps `linear_fit` by taking an additional parameter $t_0$ (or parameters) and returning the predicted value $y(t_0)$ in service of your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a20a1a9-ffe6-4462-98ba-31b3da87e9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_fit():\n",
    "    #YOUR CODE HERE\n",
    "    return\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed49d0c-a74a-4755-a297-77cb87d6c7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "t=np.array([0,1,3,6.])\n",
    "y=np.array([2,4,7,12.])\n",
    "linear_fit(t,y)\n",
    "#desired output:\n",
    "#line of best fit is y= 2.1428571428571432  +  1.6428571428571428 t\n",
    "#(2.1428571428571432, 1.6428571428571428)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec763eb-832a-4825-a709-d1b7c78c2614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 5.5.3\n",
    "# year:   1989   1990   1991   1992   1993   1994   1995   1996   1997   1998   1999\n",
    "# amount: 86.4   89.8   92.8   96.0   99.6   103.1  106.3  109.5  113.3  120.0  129.5\n",
    "#optional helper function:\n",
    "def linear_predict(t,y,t_0):\n",
    "    a,b=linear_fit(t,y)\n",
    "    return a +b*t_0\n",
    "#(or even)\n",
    "def linear_predict2(t,y,*t_m):\n",
    "    a,b=linear_fit(t,y)\n",
    "    return [(t_0,a +b*t_0) for t_0 in t_m]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a28b00-4ac9-4294-9b45-e805bfa90aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=np.array([1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999],\"float64\")\n",
    "y=np.array([86.4,89.8,92.8,96.0,99.6,103.1,106.3,109.5,113.3,120.0,129.5],\"float64\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dea6be-ee9d-411e-b85d-45e9c25d8bb7",
   "metadata": {},
   "source": [
    "**(b):**\n",
    "Now, read the second portion of section 5.5 on pages 259-260 through theorem 5.60.\n",
    "Write a function `poly_fit` which implements the process for finding a line of best polynomial fit for a set of $t$-values `t` and a set of $y$-values `y`.\n",
    "Your function should take as an additional input an integer `n` and a parameter `t_0`.\n",
    "Return the result of inputting `t_0` to the polynomial giving the degree-`n` line of best fit.\n",
    "Do this using least squares by implementing the Vandermonde matrix presented in the text (probably as its own function).\n",
    "\n",
    "*Challenge (optional):* include code which prints the formula for the line of best fit in human-readable form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9444163f-b199-4043-a73e-08af351428b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_fit():\n",
    "    #YOUR CODE HERE\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01bfe77-b8a9-4439-abf6-47b5208d4347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "t=np.array([1,2,3,4],\"float64\")\n",
    "y=np.array([-1,2,-3,4],\"float64\")\n",
    "poly_fit(2,t,y,2.5)\n",
    "# desired output:\n",
    "# -0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b65a3d9-f63c-46c2-960f-4b70a3339b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing part 2\n",
    "#testing\n",
    "t=np.array([1,10,10**2,10**3,10**4],\"float64\")\n",
    "y=np.array([-1,1,-3,4,0],\"float64\")\n",
    "poly_fit(3,t,y,10**(2.5))\n",
    "# desired output:\n",
    "# -6.615226941131597"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aadd890-3a27-4018-bf12-f9aedd3471e9",
   "metadata": {},
   "source": [
    "**(c):** Try the example below. What happens and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbcb302-6c7f-42be-a9bd-c8a272e939e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "t=np.array([1,2,3,4],\"float64\")\n",
    "y=np.array([-1,2,-3,4],\"float64\")\n",
    "poly_fit(4,t,y,np.sqrt(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786e29b6-677c-4f1a-8c40-5ae4a27605f7",
   "metadata": {},
   "source": [
    "**Exercise:** Correct this issue. Give a function `poly_fit_safe` which has the same behavior except avoiding this pitfall. Your function should always return an approximation of the greatest reasonable degree less than or equal to `n`.\n",
    "\n",
    "<details>\n",
    "    <summary>\n",
    "        <b> Hint:</b> (Click here to open)\n",
    "    </summary>\n",
    "    \n",
    "- The issue is that there are *many* possible lines of best fit when we ask for one of too high degree. Correct this issue by simply changing the degree to the greatest reasonable choice when that happens.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa18c9b8-a3be-46db-9759-8c5f04a47cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_fit_safe():\n",
    "    #YOUR CODE HERE\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942885bb-8945-4b0d-875e-dc26c975be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "t=np.array([1,2,3,4],\"float64\")\n",
    "y=np.array([-1,2,-3,4],\"float64\")\n",
    "poly_fit_safe(4,t,y,np.sqrt(2))\n",
    "#desired output: 2.495791138430997"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1225d4-dfbc-429a-8c10-c75afa86d3d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
